{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Ensemble Learning\n",
    "What are our learning objectives for this lesson?\n",
    "* Introduce ensemble learning\n",
    "    * Bagging\n",
    "    * Random Forests\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "Clone EnsembleFun from U7-Ensemble-Learning on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * PA7 is due Wednesday. Questions?\n",
    "    * Demo of (one of the) project bonus: https://interview-flask-app.herokuapp.com/\n",
    "        * See [APIServiceFunWithGUIForm](https://github.com/GonzagaCPSC322/U6-APIs-Model-Deployment/tree/master/APIServiceFunWithGUIForm) for starter code\n",
    "* Ensemble learning notes\n",
    "* IQ8 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "Basic Idea\n",
    "* Different (individual) classifiers have strengths and weaknesses\n",
    "* Instead of using just one, apply multiple (different) classifiers\n",
    "* Use voting to select prediction\n",
    "* Can lead to better prediction results\n",
    "\n",
    "Example\n",
    "* Entropy tends to result in smaller decision trees\n",
    "* Which often lead to \"better\" predictions ... but not guaranteed\n",
    "* Instead use different attribute selection approaches, and choose \"most popular\" prediction (among the ensemble)\n",
    "\n",
    "## Approaches\n",
    "Types of classification used\n",
    "* If same type (e.g., decision trees), ensemble is **homogeneous** (our focus)\n",
    "* If different types, the ensemble is **heterogeneous**\n",
    "\n",
    "There are many ways to generate many (e.g., 100s of) classifiers from data... either use all or select some for creating the ensemble\n",
    "\n",
    "Techniques we'll look at\n",
    "* Bagging (Bootstrap Aggregation)\n",
    "* Random Forests \n",
    "\n",
    "Different possible voting schemes for combining ensemble results\n",
    "* (Simple) Majority voting\n",
    "* Weighted voting\n",
    "\n",
    "### Bagging (Bootstrap Aggregation)\n",
    "Recall: Bootstrap method performs sampling with replacement\n",
    "* Given a dataset $D$ of size $|D|$ instances\n",
    "* Randomly select $|D|$ instances with replacement\n",
    "* Results in a training set approximately 63% the size of $|D|$ and a test set 36% the size of $|D|$\n",
    "* Training set will (likely) have duplicates\n",
    "\n",
    "Basic Idea\n",
    "* Generate $k$ classifiers\n",
    "* Each classifier $M_i$ is trained on $D_i$ (for $1 \\leq i \\leq k$)\n",
    "* Where $D_i$ is a bootstrap sample\n",
    "\n",
    "To classify an instance $X$:\n",
    "* Run each classifier Mi on X to get predicted label $L_i$\n",
    "* Each label $L_i$ is a vote for that label\n",
    "* Use the majority label (i.e., the mode) as the prediction\n",
    "\n",
    "### Lab Task 1\n",
    "Write a bootstrap function to return a random sample of rows with\n",
    "replacement:\n",
    "\n",
    "```python\n",
    "def bootstrap(table):\n",
    "    return [table[randint(0,len(table)-1)] for _ in len(table)]\n",
    "```\n",
    "\n",
    "Note: `randint(i, j)` returns $n$ such that $i \\leq n \\leq j$\n",
    "\n",
    "Note that instead of using bootstrapping for testing ...\n",
    "* We are using it here to **create** the ensemble for prediction\n",
    "* i.e., our classifier = set of classifiers over subsamples of original dataset\n",
    "* We are not using bootstrapping for testing in this case\n",
    "\n",
    "Some advantages of bagging (bootstrap aggregation)\n",
    "* Simple idea, simple to implement\n",
    "* Can help deal with overfitting and noisy data (outliers)\n",
    "* Can increase accuracy by reducing variance of individual classifiers\n",
    "\n",
    "### Random Forests\n",
    "Basic Idea\n",
    "* Generate many different decision trees (a \"forest\" of trees) ... $N$ trees\n",
    "\n",
    "Q: What are ways we could do this?\n",
    "* Use bagging (bootstrap aggregation)\n",
    "* Randomly select attributes (many possible trees!)\n",
    "* Use different attribute selection approaches (Entropy, GINI, ...)\n",
    "* Use a subset of attributes for each tree\n",
    "* And so on\n",
    "\n",
    "Random Forests approach:\n",
    "* Build each tree using bagging (so different data sample used for each tree)\n",
    "* At each node, select attribute from a random subset of available attributes... subset size $F$\n",
    "* Use entropy to select attribute to (split) partition on\n",
    "* Select the \"best\" subset of random trees to use in ensemble ... $M \\subset N$\n",
    "\n",
    "Note that $N$, $M$, and $F$ are all parameters of the algorithm\n",
    "\n",
    "### Lab Task 2\n",
    "Define a python function that selects F random attributes from an attribute list\n",
    "\n",
    "```python\n",
    "def random_attribute_subset(attributes, F):\n",
    "    # shuffle and pick first F\n",
    "    shuffled = attributes[:] # make a copy\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled[:F]\n",
    "```\n",
    "* `shuffle()` performs in-place rearrangement (permutation) of given sequence\n",
    "\n",
    "### The Random Forest Procedure\n",
    "1. Divide $D$ into a test and remainder set\n",
    "    * Take 1/3 for test set, 2/3 for remainder set\n",
    "    * Ensure test set has same distribution of class labels as $D$ (\"stratified\")\n",
    "    * Randomly select instances when generating test set\n",
    "2. Create $N$ bootstrap samples from remainder set\n",
    "    * Each results in a **training** (63%) and **validation** (36%) set\n",
    "    * Build and test a classifier for each of the N bootstrap samples\n",
    "    * Each classifier is a decision tree using $F$-sized random attribute subsets\n",
    "    * Determine accuracy of classifier using validation set\n",
    "3. Pick the $M$ best classifiers generated in step 2\n",
    "4. Use test set from step 1 to determine performance of the ensemble of $M$ classifiers (using simple majority voting)\n",
    "\n",
    "Again note: $N$, $M$, and $F$ are parameters (in addition to $D$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 3 (For Extra Practice)\n",
    "Assume we have a dataset with 4 attributes ($a_1$, $a_2$, $a_3$, $a_4$) where each attribute has two possible values ($v_1$ and $v_2$) and attribute $a_5$ contains class labels with two possible values ($yes$ and $no$). Using random attribute subsets of size 2:\n",
    "1. Give an example of a complete decision tree that could be generated using the random forest approach\n",
    "1. Show the random attribute subset for each attribute node in the tree."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
